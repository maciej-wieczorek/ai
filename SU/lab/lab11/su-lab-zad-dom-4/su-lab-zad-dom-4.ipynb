{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systemy uczące się - Zad. dom. 4: Boosting gradientowy\n",
    "\n",
    "### Autor rozwiązania: Maciej Wieczorek, 148141\n",
    "\n",
    "Ten notebook zawiera zadania związane z boostingiem gradientowym (Gradient Boosting)\n",
    "Do notebooka zostały dołączony plik helpers.py , które są używane w zadaniach, \n",
    "nie musisz do nich zaglądać ani ich modyfikować. Notebook jest sprawdzany półautomatycznie - przed wysyłką sprawdź czy cały kod wykonuje się bez błędów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1.\n",
    "\n",
    "W tym zadaniu będziesz implementował(a) algorytm Gradient Boosting Trees dla problemu regresji. Aby zaimplementować ten algorytm dostępny jest obiekt `Node` implementujący drzewo regresyjne. Jest to odpowiednio dostosowany obiekt, który implementowałeś w pierwszym zadaniu domowym. Możesz wykorzystać swoją własną implementację (i dostosować ją wg. opisu poniżej) lub skorzystać z implementacji w pliku `helpers`.\n",
    "\n",
    "W stosunku do poprzedniej implementacji obiekt ma pewne dodatkowe cechy, które umożliwią sprawniejszą implementację:\n",
    "- W konstruktorze `Node` jest teraz jeden obowiązkowy argument `calculate_leaf_value`, do którego należy wstawić funkcję, która jest wywoływana przez algorytm w momencie tworzenia liścia celem obliczenia jego wartości. W standardowym drzewie regresji, algorytm tworzący liść oblicza jego wartość jako średnią wartość jego elementów. Jeśli chcielibyśmy uzyskać takie działanie powinniśmy zaimplementować następującą funkcję:\n",
    "```python\n",
    "def mean_val_leaf(X, y, last_predicted):\n",
    "    return np.mean(y)\n",
    "\n",
    "tree = Node(calculate_leaf_value=mean_val_leaf)\n",
    "```\n",
    "Zwróć uwagę na parametry funkcji tworzącej lisć: `X`, `y` charakteryzujące obiekty w liściu oraz `last_predicted` przechowujące aktualną predykcję klasyfikatora dla tych obiektów. Poprzez aktualną predykcję rozumiemy tu predykcję uzyskaną wszystkimi dotychczas stworzonymi klasyfikatorami bazowymi w GBT (czyli wynik osiągnięty pozostałymi drzewami niż to tworzone). Argument `last_predicted` na chwilę obecną wydaje się niepotrzebny lecz będzie on użyty do realizacji zadania.\n",
    "\n",
    "- Dodatkowe argumenty obsługuje też funkcja ucząca model `fit(X, y, last_predicted, max_depth = None)` - która dostaje na wejście wcześniej wspomiane `last_predicted` oraz argument `max_depth` wstrzymujący budowę zbyt głębokich drzew. Innych mechanizmów pruningu niezaimplementowano, jeśli jednak takowe istnieją w Twojej implementacji, możesz je wykorzystać.\n",
    "\n",
    "Stwórz zbiór danych do regresji poniższym kodem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(-5,5,100)\n",
    "y = 5 + X + np.sin(X) + np.random.normal(scale=0.1, size=100)\n",
    "plt.plot(X,y,'o')\n",
    "X = np.expand_dims(X, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0ab39132c7a3a8f6a06d3d32f14d9b2",
     "grade": false,
     "grade_id": "cell-b262950717511628",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Zaimplementuj algorytm GBT dla błędu kwadratowego. Aby to zrobić należy uzupełnić w ogólnym pseudokodzie przedstawionym na zajęciach trzy elementy:\n",
    "- model początkowy $F_0(x)$ zwracający stałą wartość $v$ która optymalizuje błąd:\n",
    "$$F_0(x) = \\arg \\min_v \\sum_{i=1}^N L(y_i, v) $$\n",
    "- wzór na wartość ujemnego gradientu tj. pseudo-rezyduum:\n",
    "$$r_i  =  - \\frac{\\partial}{\\partial \\hat{y_i}} L(y_i, \\hat{y_i}) $$\n",
    "gdzie $\\hat{y_i}$ to aktualna predykcja klasyfikatora tj. w $m$-tej iteracji $\\hat{y_i}=F_m(x)$\n",
    "- wzór na wartość liścia $v$ optymalizujący funkcję celu całego modelu GBT\n",
    "$$v = \\arg \\min_v \\sum_{i=1}^{N_l} L(y_i, F_{m-1}(x_i) + v) $$\n",
    "Zwróć uwagę, że suma iteruje tylko po instancjach w liściu (${N_l}$ to liczba elementów w liściu).\n",
    "\n",
    "Wyznacz powyższe wartości (rozwiązania dla referencji znajdziesz poniżej komórek z kodem) i zaimplementuj algorytm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dda6f1fa44c0aafbc3a3038a204a44ac",
     "grade": false,
     "grade_id": "cell-ac1ee19f7896664e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from helpers import Node\n",
    "\n",
    "class GBTRegressor(object):\n",
    "    def __init__(self, lr=1.0):\n",
    "        self.lr = lr\n",
    "        self.initial_model = None\n",
    "        self.trees = []\n",
    "        self.residuals = []\n",
    "        \n",
    "    def fit(self, X, y, M = 1000, max_depth = 1):\n",
    "        \"\"\"\n",
    "\t\tArgumenty:\n",
    "\t\tX -- numpy.ndarray, macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech\n",
    "\t\ty -- numpy.ndarray, wektor klas o długości n, gdzie n to liczba przykładów\n",
    "        M -- int, liczba drzew\n",
    "        max_depth -- int, maksymalna głębokość pojedynczego drzewa\n",
    "\t\t\"\"\"\n",
    "        # TWÓJ KOD TUTAJ\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "\t\tArgumenty:\n",
    "\t\tX -- numpy.ndarray, macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech, dla których chcemy dokonać predykcji\n",
    "\n",
    "\t\tWartość zwracana:\n",
    "\t\tnumpy.ndarray, wektor przewidzoanych klas o długości n, gdzie n to liczba przykładów\n",
    "        \"\"\"\n",
    "        Y = np.array([f.predict(X) for f in self.trees])\n",
    "        return self.initial_model + self.lr * Y.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przetestuj działanie algorytmu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(-5, 5, num=500)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "gbt = GBTRegressor()\n",
    "gbt.fit(X, y)\n",
    "y_pred = gbt.predict(X_test)\n",
    "\n",
    "plt.plot(X,y,'o')\n",
    "plt.plot(X_test,y_pred,'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narysuj wynik modelu z odpowednio 1, 2, 5, 10 i 100 klasyfikatorami bazowami. Za klasfikator bazowy przyjmij decision stump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ade20515d1f06a84104be6a16716aaa0",
     "grade": false,
     "grade_id": "cell-8cf805d58cd501f0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'o', label='test')\n",
    "\n",
    "for m in [1, 2, 5, 10, 100]:\n",
    "    # TWÓJ KOD TUTAJ\n",
    "    \n",
    "    plt.plot(X_test, Y_test, '-', label=f'M = {m}')\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź jak zmieniają się wartości rezyduów w kilku początkowych iteracjach GBT. Narysuj wykresy $x$ vs $y-\\hat{y}$ - zwróć uwagę, że tak właśnie wyglądają zbiory na których uczą się kolejne klasyfikatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dc4e4f10861d42ad678489a7bfd866a",
     "grade": false,
     "grade_id": "cell-c812e92b907ca3a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "gbt = GBTRegressor()\n",
    "M = 10\n",
    "gbt.fit(X, y, M=M)\n",
    "cm = plt.get_cmap('viridis')\n",
    "plt.vlines(X, ymin=y, ymax=y+gbt.residuals[-1], colors='r', ls='dotted', label='last residual')\n",
    "for i in range(M):\n",
    "    color = cm(np.ones(len(X)) * i/(M-1))\n",
    "    plt.scatter(X, y + gbt.residuals[i], c=color, s=10, label=f'iter = {i+1}')\n",
    "plt.scatter(X, y, c='r', s=50, marker='*', label='ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Odpowiedzi:*\n",
    "- model początkowy $F_0(x)$\n",
    "$$F_0(x) = \\arg \\min_v \\sum_{i=1}^N L(y_i, v) = \\frac{1}{2} \\sum_{i=1}^N (y_i- v)^2 $$\n",
    "wartość ta to oczywiście średnia arytmetyczna $v = \\frac{1}{n} \\sum_{i=1}^N y_i$. (Upewnij się, że to rozumiesz poprzez policzenie pochodnej i przyrównanie jej do 0).\n",
    "- wzór na wartość ujemnego gradientu tj. pseudo-rezyduum \n",
    "$$r_i  =  - \\frac{\\partial}{\\partial \\hat{y_i}} L(y_i, \\hat{y_i}) = - \\frac{\\partial}{\\partial \\hat{y_i}} \\frac{1}{2}(y_i- \\hat{y_i})^2$$\n",
    "Co po przekształceniach wykorzystujących regułę łańcuchową (\"pochodna zewnętrzna razy pochodna wewnętrzna\"):\n",
    "$$r_i  = -\\frac{1}{2} 2(y_i- \\hat{y_i})\\frac{\\partial}{\\partial \\hat{y_i}} (y_i- \\hat{y_i}) \n",
    "= -(y_i- \\hat{y_i})\\cdot(-1)\n",
    "= y_i- \\hat{y_i}  $$\n",
    "- wzór na wartość liścia $v$ optymalizujący funkcję celu\n",
    "$$v = \\arg \\min_v \\sum_{i=1}^{N_l} L(y_i, F_{m-1}(x_i) + v) = \\frac{1}{2} \\sum_{i=1}^{N_l} (y_i- F_{m-1}(x_i) - v)^2 $$\n",
    "Co można dalej obliczyć poprzez przyrównanie pochodnej do 0 lub poprzez zauważenie że jest to w naszej sytuacji ten sam wzór co dla modelu początkowego gdzie $y_i$ zostało zastępione $y_i- F_{m-1}(x_i)=r_i$. W związku z tym wartość liścia to $v = \\frac{1}{n} \\sum_{i=1}^N r_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Zadanie 2.\n",
    "\n",
    "Zaimplementuj GBT dla problemu klasyfikacji binarnej, który będzie optymalizował błąd regresji logicznej tj. entropię krzyżową wyrażoną wzorem:\n",
    "$$L(y_i, \\hat{p_i}) = y_i \\log \\hat{p_i} +  (1-y_i) \\log (1-\\hat{p_i}) $$\n",
    "gdzie $y_i\\in \\{0,1\\}$ to prawdziwa wartość klasy a $\\hat{p_i}$ to predykcja klasyfikatora dla $i$-tego elementu.\n",
    "\n",
    "- Zauważ, że GBT wykorzystuje drzewa regresji, które - choć modyfikujemy im sposób obliczania liści - nadal tworzą podziały dla miary SSE. Aby wykorzystać GBT do problemu klasyfikacji należy zastanowić się jak możemy wykorzystać regresor do klasyfikacji. Ten problem rozwiązywaliśmy już wcześniej przy omawianiu regresji logistycznej, gdzie tworzyliśmy klasyfikator z modelu regresji liniowej. Przypomnijmy, że w regresji logistycznej model regresji liniowej służy do predykcji logitu prawdopodobieństwa klasy (który przypomnijmy ma zakres wartości od $-\\infty$ do $\\infty$)\n",
    "$$\\text{logit}(p_x) = \\ln \\frac{p_x}{1-p_x}=w^Tx+b$$\n",
    "Podobnie w GBT należy skonstruować model regresji do przewidywania wartości $\\text{logit}(p_x)$, a jedynie przy predykcji (lub kiedy jest to wygodne) transformować go do prawdopodobieństwa klasy funkcją sigmoidalną $p_x  = \\frac{1}{1+e^{- \\text{logit}(p_x)}}  $\n",
    "\n",
    "**Zadania**\n",
    "\n",
    "1. Powyższy zapis funkcji celu $L(y_i, \\hat{p_i})$ jest wyrażony w zależności od prawdopodobieństwa klasy, a nie wartości logitu $L(y_i, \\text{logit}(\\hat{p_i}))$. Przekształć wzór na funkcję celu, aby jej argumentem był logit. Zwróć uwagę, że model regresji będzie przewidywał właśnie logit, więc przy wyznaczaniu elementów algorytmu GBT należy liczyć np. pochodne tej właśnie przekształconej funkcji.\n",
    "\n",
    "    Zapisz wzór na tę funkcję w komórce poniżej:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e95a2d0d1d41c55f2535e921b22a1ee6",
     "grade": true,
     "grade_id": "cell-7cd53887e8606afb",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$$\\hat y_i \\equiv \\text{logit}(\\hat p_i)$$\n",
    "\n",
    "$$\\hat p_i = \\frac{ 1 }{ 1 + e^{- \\hat y_i} } = \\frac{ e^{\\hat y_i} }{ 1 + e^{\\hat y_i} }$$\n",
    "\n",
    "$$L(y_i, \\hat y_i) = ?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Zacznijmy uzupełniać w ogólnym pseudokodzie przedstawionym na zajęciach brakujące elementy. Wyznacz model początkowy $F_0(x)$ zwracający stałą wartość $v$ która optymalizuje błąd:\n",
    "$$F_0(x) = \\arg \\min_v \\sum_{i=1}^N L(y_i, v) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b91c1d7e3c8ef74aac7a737349627589",
     "grade": true,
     "grade_id": "cell-a31084a299567b73",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$$\\sum_{i=1}^n \\frac{\\partial}{\\partial v} L(y_i, v) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Wyznacz wzór na wartość ujemnego gradientu tj. pseudo-rezyduum:\n",
    "\n",
    "$$r_i  =  - \\frac{\\partial}{\\partial \\hat{y_i}} L(y_i, \\hat{y_i}) $$\n",
    "Uwaga 1: na samym końcu, aby wzór uzykał prostszą formę, możesz zamienić w nim wartości logitów z powrotem na prawdopodobieństwa. \n",
    "\n",
    "Uwaga 2: musiałeś policzyć go ju w poprzednim punkcie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15674c47ecf694084ebcf9c3f575fe23",
     "grade": true,
     "grade_id": "cell-dc1c684d06b2f089",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$$r_i = ?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Wzór na wartość liścia $v$ optymalizujący funkcję celu całego modelu GBT\n",
    "$$v = \\arg \\min_v \\sum_{i=1}^{N_l} L(y_i, F_{m-1}(x_i) + v) $$\n",
    "niestety nie jest prosty do wyznaczenia w tym przypadku. Stosuje się przybliżenie Taylora drugiego rzedu tej funkcji i wtedy optimum ma postać:\n",
    "$$v = \\frac{-\\sum_{i=1}^{N_L} L_i' }{\\sum_{i=1}^{N_L} L_i''}$$\n",
    "gdzie $L_i'$ i $L_i''$ to skrócony zapis pierwszej i drugiej pochodnej policzonej po funkcji straty dla $i$-tego elementu. Ponieważ $r_i=-L_i'$ to licznik przyjmuje postać $\\sum_{i=1}^{N_L} r_i $. Wyznacz cały wzór.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85ac93a3a935146b87119789893eb706",
     "grade": true,
     "grade_id": "cell-193f2492d4d4966d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$$\\hat{y_i} \\equiv F_{m-1}(x_i)$$\n",
    "\n",
    "$$\n",
    "v = \\frac{-\\sum_{i=1}^n \\frac{\\partial}{\\partial v} L(y_i, \\hat{y_i} + v)}{\\sum_{i=1}^n \\frac{\\partial^2}{\\partial^2 v} L(y_i, \\hat{y_i} + v)} = ?\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykorzystując uzyskane wyniki zaimplementuj algorytm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10d00855082fd2660203fa55f0192e12",
     "grade": false,
     "grade_id": "cell-49860f3147428f2e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "# Wskazówka: scipy.special.expit() implemenuje funkcję sigmoidalną\n",
    "\n",
    "class GBTClassifier(object):\n",
    "    def __init__(self, lr=1.0):\n",
    "        self.lr = lr\n",
    "        self.initial_model = None\n",
    "        self.trees = []\n",
    "        self.residuals = []\n",
    "        \n",
    "    def fit(self, X, y, M=100, max_depth=1):\n",
    "        \"\"\"\n",
    "\t\tArgumenty:\n",
    "\t\tX -- numpy.ndarray, macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech\n",
    "\t\ty -- numpy.ndarray, wektor klas o długości n, gdzie n to liczba przykładów\n",
    "        M -- int, liczba drzew\n",
    "        max_depth -- int, maksymalna głębokość pojedynczego drzewa\n",
    "\t\t\"\"\"\n",
    "        # TWÓJ KOD TUTAJ\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "\t\tArgumenty:\n",
    "\t\tX -- numpy.ndarray, macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech, dla których chcemy dokonać predykcji\n",
    "\n",
    "\t\tWartość zwracana:\n",
    "\t\tnumpy.ndarray, wektor przewidzoanych klas o długości n, gdzie n to liczba przykładów\n",
    "        \"\"\"\n",
    "        Y = np.array([f.predict(X) for f in self.trees])\n",
    "        return scipy.special.expit(self.initial_model + self.lr * Y.sum(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1d9196e9bcfdf96ee378612cc201a26",
     "grade": false,
     "grade_id": "cell-5e0ac3aa1b4504ff",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Przetestuj swoją implementację na zbinaryzowanym zbiorze `iris`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "y[y==2] = 0 # Sprowadzenie problemu do klasyfikacji binarnej\n",
    "\n",
    "def draw_boundary(X, y, M=100, max_depth=1, lr=1, ax=None):\n",
    "    # Kod rysowania\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "    clf = GBTClassifier(lr=lr)\n",
    "    clf.fit(X, y, M=M, max_depth=max_depth)\n",
    "    Y = clf.predict(X)\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n",
    "    loss = -np.mean(y*np.log(Y) + (1 - y)*np.log(1 - Y))\n",
    "    ax.set_title(f'M={M}, max_depth={max_depth}, lr={lr}, L={loss:.4f}')\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "draw_boundary(X, y, M=100, max_depth=1, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narysuj granice decyzji klasyfikatora dla 10, 20, 50 i 100 iteracji algorytmu dla klasyfikatora bazowego o maksymalnej głębokości 1 i 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c9df781f941bc16c82c578aab5cc772",
     "grade": false,
     "grade_id": "cell-23a29d3c5f38cd20",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(15, 20))\n",
    "for i, M in enumerate([10, 20, 50, 100]):\n",
    "    for j, max_depth in enumerate([1, 2]):\n",
    "        draw_boundary(X, y, M=M, max_depth=max_depth, ax=axs[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polecenia**\n",
    "\n",
    "1. W jaki sposób zaimplementować GBT dla problemu klasyfikacji wieloklasowej?\n",
    "2. W powyższym problemie który z klasyfikatorów bazowych (o jakiej max. głębokości) poradził sobie lepiej? Czy jest sens stosować w tym problemie drzewa o głębokości większej niż testowana (tj. 2). Odpowiedź uzasadnij.\n",
    "3. Dodaj do implementacji parametr $\\eta$ i przetestuj kilka jego wartości. Pamętaj, że $\\eta$ powinno być wykorzystywane nie tylko w funkcji `fit`, ale także `predict` - dlaczego?\n",
    "\n",
    "Odpowiedź na drugą kropkę umieść poniżej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(MIEJSCE NA TWOJE ODPOWIEDZI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "911136fc384b65753685caf4d29dd32e",
     "grade": true,
     "grade_id": "cell-73f4aa08b5f50458",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=2)\n",
    "for ax, lr in zip(axs.reshape(-1), [1, 0.5, 0.1, 0.01]):\n",
    "    draw_boundary(X, y, M=100, max_depth=1, lr=lr, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3.\n",
    "\n",
    "GBT jest popularnym algorytmem między innymi dzięki bardzo efektywnym implementacjom potrafiącym sobie radzić z dużymi zbiorami danych. W tym ćwiczeniu Twoim zadaniem jest nauczenie się podstaw obsługi biblioteki `catboost`, którą powinieneś zainstalować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytanie zbioru danych `iris` z poprzedniego zadania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "y[y==2] = 0 # Sprowadzenie problemu do klasyfikacji binarnej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trening modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model = CatBoostClassifier(logging_level='Silent')\n",
    "model.fit(X, y, eval_set=(X, y), plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykładowy kod ewaluuje działanie algorytmu na części uczącej. Podziel zbiór na część uczącą i testową i ponownie uruchom algorytm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2)\n",
    "model = CatBoostClassifier(logging_level='Silent')\n",
    "model.fit(X_tr, y_tr, eval_set=(X_te, y_te), plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dla chętnych*: porównaj wartość funkcji straty osiągniętej przez catboost z Twoją implementacją z zadania 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimportuj dowolny większy i bardziej wymagający zbiór danych. Ćwiczenie możesz wykonać na [dowolnym zbiorze danych](https://catboost.ai/docs/concepts/python-reference_datasets.html) - ładowanie zbioru może trochę potrwać. Jeśli masz problemy sprzętowe z operowaniem na dużym zbiorze danych to jest też dostępny zbiór `titanic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import datasets\n",
    "\n",
    "ds_tr, ds_te = datasets.titanic()\n",
    "\n",
    "# TWÓJ KOD TUTAJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4008306dfc33868b6dac96ffdf560f82",
     "grade": false,
     "grade_id": "cell-05da6f295da51eef",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Spróbuj osiągnąć jak najlepszy wynik na wybranym zbiorze poprzez tuning parametrów. Ważne parametry uczenia zostały opisane [tutaj](https://catboost.ai/docs/concepts/python-reference_parameters-list.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(logging_level='Silent')\n",
    "\n",
    "grid = dict(\n",
    "    learning_rate=[0.1, 1.0],\n",
    "    depth=[4, 6, 10],\n",
    "    # ???\n",
    ")\n",
    "result = model.randomized_search(grid, X_tr, y_tr, plot=True)\n",
    "# `model` może być teraz użyty do predykcji \n",
    "print(result.params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
