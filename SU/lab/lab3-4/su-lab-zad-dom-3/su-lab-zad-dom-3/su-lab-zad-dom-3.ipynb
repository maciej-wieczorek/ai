{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systemy uczące się - Zad. dom. 2: Maszyny wektorów podpierających\n",
    "\n",
    "### Autor rozwiązania: Maciej Wieczorek 148141\n",
    "\n",
    "Ten notebook zawiera zadania związane z maszynami wektorów podpierających (SVM).\n",
    "Do notebooka zostały dołączony plik helpers.py oraz dwa pliki .npz z danymi, które są używane w zadaniach, \n",
    "nie musisz do nich zaglądać ani ich modyfikować. Notebook jest sprawdzany półautomatycznie - przed wysyłką sprawdź czy cały kod wykonuje się bez błędów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1. - Implementacja maszyny wektorów podpierających (za 20%)\n",
    "\n",
    "Zadanie polega na zaimplementowaniu maszyny wektorów podpierających z wykorzystaniem biblioteki [`cvxpy`](https://github.com/cvxpy/cvxpy) zapewniającej solver do programowania kwadratowego. Rozpocznijmy od szybkiej nauki jak ta biblioteka działa:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cvxpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optymalizacja bez ograniczeń: znajdź maksimum funkcji $$ - (x - 5 )^2 + 10$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxpy import *\n",
    "x = Variable(1) #zmienna optymalizowana, wektor o długości 1 (bo to jedna liczba)\n",
    "\n",
    "objective = Maximize(-(x-5)**2 + 10) # Implementacja funkcji celu ze wskazanym kierunkiem maksymalizacji\n",
    "prob = Problem(objective) # Stworzenie problemu optymalizacji\n",
    "\n",
    "print(prob.solve()) #Rozwiąż i zwróć wartość funkcji celu\n",
    "print(x.value) # Znaleziona wartość x\n",
    "print(prob.status) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optymalizacja z ograniczeniami: znajdź minimum funkcji $e^{(x-2)^2}$ dla $x>0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(1)\n",
    "\n",
    "objective = Minimize(exp((x-2)**2))\n",
    "constraints = [x>=0]\n",
    "prob = Problem(objective, constraints) # Problem teraz składa się z funkcji celu i *listy* ograniczeń\n",
    "\n",
    "print(prob.solve())\n",
    "print(x.value)\n",
    "print(prob.status) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optymalizacja z ograniczeniami: znajdź minimum funkcji $x + y$ dla $x\\geq 0, y\\geq 6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(1)\n",
    "y = Variable(1)\n",
    "\n",
    "objective = Minimize(x + y)\n",
    "constraints = [x>=0, y>=6] # <- To zwykła pythonowa lista\n",
    "prob = Problem(objective, constraints) # Problem teraz składa się z funkcji celu i listy ograniczeń\n",
    "\n",
    "print(prob.solve())\n",
    "print(x.value, y.value)\n",
    "print(prob.status) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatywna postać tego samego problemu, wykorzystująca notację wektorową:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(2)\n",
    "\n",
    "objective = Minimize(sum(x))\n",
    "constraints = [x[0]>=0, x[1]>=6]\n",
    "prob = Problem(objective, constraints) # Problem teraz składa się z funkcji celu i listy ograniczeń\n",
    "\n",
    "print(prob.solve())\n",
    "print(x.value)\n",
    "print(prob.status) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygenerujmy i narysujmy próbkę dwuwymiarowych danych, które są separowalne liniowo. Zwróć uwagę, że dla uproszczenia przyszłej implementacji każda z cech jest przechowywana w osobnym wektorze. Wektor `x` zawiera wartości pierwszej cechy, wektor `y` zawiera wartości drugiej cechy, a `labels` zawiera wartości klasy decyzyjnej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import plot_classification, get_separable\n",
    "x,y,labels = get_separable()\n",
    "plot_classification(x,y,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy dane pochodzące z dwóch klas, a nasze zadanie polega na implementacji klasyfikatora SVM czyli indukcji (w naszym dwuwymiarowym przypadku) linii któa będzie separowała obie klasy. \n",
    "$$a \\cdot x+b \\cdot y +c = 0$$\n",
    "Klasyfikator będzie przypisywał obiekt do jednej z klas gdy będzie się on znajdował po prawej stronie tej linii, jeśli zaś obiekt będzie po lewej jej stronie to przypiszemy obiekt do drugiej z klas. Znajdź taką linię dla powyższych danych, gdzie `x` i `y` to współrzędne punktów, a `labels` przyjmuje wartości $1$ i $-1$ wskazując na klasę obserwacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b84d6c082e52e22080969feb1bbafd1b",
     "grade": false,
     "grade_id": "cell-26cf5a59f7e76719",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def construct_svm_opt_problem(xs, ys, labels):\n",
    "    a, b, c = Variable(), Variable(), Variable()\n",
    "    objective = Minimize(a**2 + b**2 + c**2)\n",
    "    indicies_one = np.where(labels==1)\n",
    "    indicies_minus_one = np.where(labels==-1)\n",
    "    constraints = [a*xs[indicies_one]+b*ys[indicies_one] + c >= 1, a*xs[indicies_minus_one]+b*ys[indicies_minus_one] + c <= -1]\n",
    "    prob = Problem(objective, constraints)\n",
    "\n",
    "    return prob\n",
    "\n",
    "prob = construct_svm_opt_problem(x, y, labels)\n",
    "prob.solve()\n",
    "prob.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli dobrze wytrenowałeś klasyfikator to powinno się udać go narysować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = prob.variables()\n",
    "plot_classification(x,y,labels, a = a.value, b=b.value, c=c.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przejdźmy na trudniejszy zbiór, który nie jest liniowo separowalny. \n",
    "\n",
    "**UWAGA** Dane są losowane - istnieje mała szansa, że akurat wylosujesz zbiór separowalny. W takiej sytuacji powtórz wywołanie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import plot_classification, get_non_separable\n",
    "\n",
    "x, y, labels = get_non_separable()\n",
    "plot_classification(x, y, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Rozszerz swoją implementację klasyfikatora SVM tak aby prawidłowo działał również ze zbiorami nieliniowo separowalnymi. Za stałą $C$ możesz przyjąć 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54c53b217e616c13008cd1a05021acbe",
     "grade": false,
     "grade_id": "cell-78a5f6e4f32cc407",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def construct_softsvm_opt_problem(xs, ys, labels, C = 1):\n",
    "    a, b, c, Eta = Variable(), Variable(), Variable(), Variable(len(labels))\n",
    "    objective = Minimize(a**2 + b**2 + c**2 + C * sum(Eta**2))\n",
    "    indicies_one = np.where(labels==1)\n",
    "    indicies_minus_one = np.where(labels==-1)\n",
    "    constraints = [a*xs[indicies_one]+b*ys[indicies_one] + c >= 1 - Eta[indicies_one], a*xs[indicies_minus_one]+b*ys[indicies_minus_one] + c <= -1 + Eta[indicies_minus_one]]\n",
    "    prob = Problem(objective, constraints)\n",
    "\n",
    "    return prob\n",
    "    \n",
    "prob = construct_softsvm_opt_problem(x, y, labels)\n",
    "prob.solve()\n",
    "prob.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narysuj nauczony klasyfikator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, Eta = prob.variables()\n",
    "plot_classification(x,y,labels, a = a.value, b = b.value, c = c.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2. - Eksploracja liniowego SVM (za 20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytaj dane liniowo sepratowalne gdzie `X` to macierz cech, a `y` zwiera wartości klasy decyzyjnej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_separable\n",
    "X, y = get_separable(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wytrenuj model maszyn wektorów podpierających używając obiektu SVC (Support Vector Classifier) z biblioteki `sklearn`. Konstruktor tego obiektu ma parametr `kernel` który domyślnie jest ustawiony na jądro RBF, początkowo będziemy chcieli używać wersji klasyfikatora bez jądra czyli inaczej z jądrem liniowym - ustaw ten parametr na wartość `\"linear\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd90bc25a918b3585993af347e85ae6e",
     "grade": false,
     "grade_id": "cell-e86700b33c9e4bf6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolejnym krokiem będzie narysowanie granicy decyzji. Funkcję możesz implementować przyrostowo, punkt po punkcie.\n",
    "1. Niech funkcja narysuje dane uczące (X,y) zaznaczając kolorem klasy decyzyjne.\n",
    "2. Zaznacz na wykresie granicę decyzji. Wytrenowany SVM (obiekt SVC) ma odpowiednie wartości nauczonych współczynników we właściwości `svm.coef_` a także wyraz wolny w `svm.intercept_`<br>\n",
    "    *Wskazówka*: Zwróć uwagę, że linię separującą o wzorze\n",
    "    $w_0*x_0 + w_1*x_1 + b = 0$ można przekształcić do postaci:\n",
    "    $$x_1= -\\frac{w_0}{w_1}  x_0 - \\frac{b}{w_1} $$\n",
    "3. Zaznacz na wykresie margines decyzji (np. poprzez narysowanie dwóch linii równoległych do granicy decyzji w odpowiedniej odległości). Przypomnij sobie, że z naszych wyprowadzeń wynika że wartość marginesu to $\\frac{1}{||w||}$\n",
    "4. Zakreśl specjalnym symbolem lub podświetl elementy zbioru uczącego będącymi wektorami podpierającymi. Ich współrzędne możesz odnaleźć we właściwości `svm.support_vectors_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05d5e12dea0a00c6c240e64d381c3728",
     "grade": true,
     "grade_id": "cell-663459d5c2152f0c",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_svm(svm, X, y):\n",
    "    \"\"\" Input: Funkcja na wejście otrzymuje wytrenowany klasyfikator SVM (obiekt SVC) \n",
    "        oraz zbiór danych (X - cechy , y - klasa)\n",
    "        Output: Funkcja rysuje dane oraz zaznacza granicę decyzji\n",
    "    \"\"\"    \n",
    "    # TWÓJ KOD TUTAJ\n",
    "    plt.scatter(X[:, 0], X[:, 1], c = y)\n",
    "    x = np.arange(X[:, 0].min(), X[:, 0].max(), 0.02)\n",
    "    w0 = svm.coef_[0][0]\n",
    "    w1 = svm.coef_[0][1]\n",
    "    b = svm.intercept_[0]\n",
    "    y_x = -x*(w0/w1) - (b/w1)\n",
    "    margin = 1 / np.linalg.norm(svm.coef_[0])\n",
    "    plt.plot(x, y_x)\n",
    "    plt.plot(x, y_x-margin, 'g--')\n",
    "    plt.plot(x, y_x+margin, 'g--')\n",
    "    plt.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], marker='+')\n",
    "    print(svm)\n",
    "    \n",
    "plot_svm(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę na granicę decyzji, narysowany margines oraz podświetlone obiekty będące wektorami podpierającymi.\n",
    "\n",
    "Wczytaj nowy zbiór danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dane = np.load('data_outlier.npz')\n",
    "X = dane['X']\n",
    "y = dane['y'].reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oraz powtórz poprzednią procedurę tj. wytrenuj klasyfikator oraz narysuj wykres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85f58f46947265626295a1073c8e6187",
     "grade": false,
     "grade_id": "cell-ce8bde467c6002e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')\n",
    "model.fit(X, y)\n",
    "plot_svm(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W `sklearn` klasyfikator SVM ma domyślną wartość parametru $C = 1.0$. Zbiór jest jednak liniowo separowalny - sprawdź jak wygląda granica decyzji z innymi wartościami $C$ a w szczególności z wartością `C = float(\"inf\")`. Wartość $C$ jest parametrem konstruktora obiektu SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ace73f42e9f6d62d1db0c53293f7c42",
     "grade": false,
     "grade_id": "cell-fc823d7c2809aced",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear', C=float('inf'))\n",
    "model.fit(X, y)\n",
    "plot_svm(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wypróbuj różne wartości parametru $C$ na zbiorze nieseparowalnym liniowo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_non_separable\n",
    "X, y = get_non_separable(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6abdd8a7564c2b969deefa69c180fd7e",
     "grade": false,
     "grade_id": "cell-333c466ccbb99e45",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear', C=0.001, probability=True)\n",
    "model.fit(X, y)\n",
    "plot_svm(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polecenia**\n",
    "1. Jak zmienia się wyindukowana granica decyzji dla bardzo dużych ($\\infty$) i bardzo małych (np. 0.01) wartości $C$? Jak zmienia się trafność klasyfikatora na zbiorze uczącym? Jak sądzisz, która granica decyzja będzie najlepsza na zbiorze testowym?\n",
    "2. Jak zmienia się liczba wektorów podpierających wraz ze zmianą $C$? Liczba wektorów podpierających jest miarą złożoności hipotezy - jakie wartości $C$ powodują bardziej złożone hipotezy? Odnieś te rozważania do przetargu wariancja - obciążenie.\n",
    "3. Czy klasyfikator SVM jest klasyfikatorem który może zwrócić prawdopodobieństwo przynależności do klas? Czy jest to klasyfikator generatywny czy dyskryminacyjny?\n",
    "\n",
    "Odpowiedz na powyższe pytania:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dla bardzo dużych C wartości zmiennych osłabiających są jak najmniejsze przez co algorytm preferuje wąskie marginesy nienaruszające prawidłowej klasyfikacji. Dla bardzo małych wartości C większość ograniczeń może zostać wyeliminowana przez co wagi modelu mogą być jak najmniejsze i margines jak największy. Na zbiorze uczącym trafność klasyfikacji będzie największa dla dużych wartości C, jednak może to powodować przesadne dostosowywanie do przypadków odstających i słabszy wynik na zbiorze testowym.\n",
    "2. Liczba wektorów podpierających wzrasta kiedy wartość C maleje. Większe wartości C dają mniej obciążony model który posiada więcej wariancji i jest bardziej podatny na dostosowanie się do przypadków odstających.\n",
    "3. Tak może zwrócić prawdopodobieństo przynależności do klasy np. biorąc po uwagę odległość od marginesu. Jest to klasyfikator dyskryminacyjny, ponieważ nie uczy się rozkładu danych, a jedynie tego jak je rozdzielać."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3. - Trik jądrowy (za 20%)\n",
    "Klasyfikator SVM, który jest liniowy jest dość mocno ograniczony. Nie będzie potrafił m.in. prawidłowo zaklasyfikować przedstawionego zbioru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=2024)\n",
    "\n",
    "model = SVC(kernel=\"linear\")\n",
    "model.fit(X, y)\n",
    "plot_svm(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem ten można rozwiązać podobnie jak w przypadku regresji logistycznej: poprzez dodawanie dodatkowych cech. Na zajęciach z regresji dodawaliśmy cechy wielomianowe: spróbuj tego tricku również teraz wykorzystując obiekt `Pipeline` oraz `PolynomialFeatures` z cechami stopnia 3. \n",
    "\n",
    "**Uwaga**: do rysowania wykresu trzeba teraz użyć funkcji `plot_classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69b001e2315d2c0bfe472cdeb559fde1",
     "grade": false,
     "grade_id": "cell-31013b2d87bad861",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from helpers import plot_classifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('cubic features', PolynomialFeatures(degree=3, include_bias=False)),\n",
    "    ('svc', SVC(kernel='linear'))\n",
    "])\n",
    "\n",
    "#call the pipeline\n",
    "pipeline.fit(X, y)\n",
    "plot_classifier(X, y, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak jak dyskutowaliśmy na laboratoriach (i wykładzie) tego typu operacja dla zbiorów które mają nawet niezbyt dużo cech może wygenerować bardzo dużo cech. Dla przykładu rozważając zbiór z tysiącem cech i generację cech wielomianowych tylko *drugiego* rzędu (czyli de facto najniższego nieliniowego) spowoduje dodanie prawie pół miliona cech! Nie mniej jednak kernel (jądro) takiej operacji jest wyrażone wzorem:\n",
    "$(x^Ty + 1 )^2$ i można je obliczyć w czasie liniowym od liczby oryginalnych cech (tutaj 1000)!\n",
    "\n",
    "W obiekcie SVC możemy wykorzystać jądro wielomianowe specyfikując `kernel=\"poly\"` oraz ew. podając dodatkowe parametry `degree` oraz `coef0`. Co oznaczają te parametry? Jądro wielomianowe jest wyrażone wzorem:\n",
    "$$K(x,y) = (x^\\mathsf{T} y + c)^{d}$$\n",
    "gdzie $d$ to właśnie `degree`, a stała $c$ to `coef0`. Zmienna $d$ ma bardzo prostą interpretację: jest to stopień wielomianu. Stała $c$ ma natomiast szczególne znaczenie: jeśli jest ona równa $c=0$ to kernel generuje wszystkie jednomiany rzędu $d$ (jądro wielomianowe homogeniczne), jeżeli $c=1$ to kernel generuje wszystkie jednomiany rzędu co najwyżej $d$. Dodatkowo stała $c$ może być też strojona (ale w praktyce raczej rzadko się to robi)  ponieważ reguluje ona przetarg między wpływem cech wyższego rzędu a wpływem cech rzędu niższego. Stała $c$ nie może być ujemna. \n",
    "\n",
    "Wykorzystaj jądro wielomianowe do klasyfikacji badanego zbioru. (Pamiętaj o wyelimiowaniu dodawania cech wielomianowych!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d284dd235beaea17e32b870dd9a62432",
     "grade": false,
     "grade_id": "cell-da224e59102ac248",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from helpers import plot_classifier\n",
    "\n",
    "model = SVC(kernel='poly', coef0=1)\n",
    "model.fit(X, y)\n",
    "plot_classifier(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM z jądrami może sobie teraz poradzić także z problemem XOR. Wygenerujmy takie dane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(200, 2)\n",
    "y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i wytrenuj klasyfikator. Spróbuj pozmieniać wartość kontrolującą złożoność hipotezy $C$ (nie jest to $c$ z funkcji jądra!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99dd2779ce0214e2e5b7e2bbfce026a1",
     "grade": false,
     "grade_id": "cell-e8a6977a244262c1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='poly', coef0=1)\n",
    "model.fit(X, y)\n",
    "plot_classifier(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteruj parametrami `degree` i `C` dla problemu XOR oraz wcześniejszego problemu klasyfikacji (dwa księżyce)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prawdopodobnie najczęściej stosowaną funkcją jądrową jest jądro RBF (*Radial basis function kernel*) wyrażone wzorem:\n",
    "$${\\displaystyle K(\\mathbf {x} ,\\mathbf {x'} )=\\exp(-\\gamma \\|\\mathbf {x} -\\mathbf {x'} \\|^{2})}$$\n",
    "który inaczej zapisany ${\\displaystyle K(\\mathbf {x} ,\\mathbf {x'} )=\\exp \\left(-{\\frac {\\|\\mathbf {x} -\\mathbf {x'} \\|^{2}}{2\\sigma ^{2}}}\\right)}$ przypomina wzór na rozkład normalny (przy $\\gamma^{-1} = 2\\sigma ^{2}$). Jego działanie możemy sobie wyobrazić jako stosowanie rozkładu normalnego, który jest wycentrowany na rozważanym przykładzie $\\mathbf{x}$ do mierzenia podobieństwa z innymi przykładami. Przypisuje on najwyższe \"prawdopodobieństwo\" (wartość funkcji jądrowej) do naszej obserwacji $\\mathbf{x}$ a im dalej od niej tym wartość spada. Jeśli więc inny przykład $\\mathbf{x'}$ leży blisko $\\mathbf{x}$ w przestrzeni cech to jądro odpowie wysoką wartością, im dalej tym odpowiedź będzie niższa. Z kształtu rozkładu normalnego wiemy też, ze po przekroczeniu pewnej odległości ($3\\sigma$) jądro będzie odpowiadało zawsze (prawie) zerem. Szybkość opadania funkcji jądra reguluje właśnie parametr $\\sigma$: im jest on wyższy, tym wariancja (szerokość) rozkładu jest wyższa, więc jądro wolniej spada. Innymi słowy przykłady mają \"większy zakres oddziałowywawania\" na inne przykłady. W kontekście jądra RBF przyjęło się sterować parametrem $\\gamma$, który jak już podawałem wynika z podstawienia:\n",
    "$$\\gamma = \\frac{1}{2\\sigma ^{2}}$$\n",
    "czyli im wyższe $\\gamma$ (niższe $\\sigma$) tym mniejszy zakres oddziałowywania, funkcja szybciej spada z odległością od przykładu i na odwrót. \n",
    "\n",
    "Niezwykłą ciekawostką dot. tej funkcji jądrowej jest to, że gdybyśmy zastanowili się jak wygląda przestrzeń cech które powinniśmy wygenerować aby dostać taki wynik bez użycia triku jądrowego to okazałoby się, że jest ich... nieskończenie wiele!\n",
    "\n",
    "Jak pewnie się domyślasz, zastąpienie w konstruktorze `SVC` wyboru jądra na `kernel=\"rbf\"` oraz podanie parametru `gamma` umożliwi ci przetestowanie SVM z jądrem RBF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aee628aa251c67567368fea730ac8892",
     "grade": false,
     "grade_id": "cell-11f5b7a4068a5b46",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.random.randn(200, 2)\n",
    "y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\n",
    "\n",
    "def plot_rbf(gamma):\n",
    "    model = SVC(kernel='rbf', gamma=gamma)\n",
    "    model.fit(X, y)\n",
    "    plot_classifier(X, y, model)\n",
    "    \n",
    "plot_rbf(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in [0.01, 0.1, 1, 10]:\n",
    "    plot_rbf(gamma)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.05)\n",
    "\n",
    "for gamma in [0.5, 0.7, 1, 1.5, 1.7, 2]:\n",
    "    model = SVC(kernel='rbf', gamma=gamma)\n",
    "    model.fit(X, y)\n",
    "    print(gamma)\n",
    "    plot_classifier(X, y, model)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(2000,2)*20\n",
    "y = np.sin(X[:,0])+np.sin(X[:,1])>0\n",
    "\n",
    "for gamma in [0.5, 0.7, 1, 1.5, 1.7, 2]:\n",
    "    model = SVC(kernel='rbf', gamma=gamma)\n",
    "    model.fit(X, y)\n",
    "    print(gamma)\n",
    "    plot_classifier(X, y, model)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polecenia**: \n",
    "1. Przetesuj kilka wartości `gamma`. Rozważ przynajmniej wartości 0.01, 0.1, 1, 10.\n",
    "2. Jakie wartości `gamma` powodują przeuczenie, a jakie niedouczenie?\n",
    "3. Wczytaj poprzedni problem z dwoma księżycami i spróbuj dobrać `gamma`.\n",
    "4. Wygeneruj zbiór z szachownicą\n",
    "    ```\n",
    "    X = np.random.rand(2000,2)*20\n",
    "    y = np.sin(X[:,0])+np.sin(X[:,1])>0\n",
    "    ```\n",
    "    i spróbuj wybrać odpowiednio sparametryzowaną funkcję jądrową.\n",
    "\n",
    "5. Czy klasyfikator SVM jest odporny na zmianę skali? Innymi słowy czy powinno się normalizować cechy przed jego użyciem? Dlaczego?\n",
    "6. Jak myślisz czy dla każdego problemu (lub ew. prawie każdego) jest możliwe takie dobranie jądra np. wielomianowego aby zbiór był liniowo separowalny? Tj. byłoby w tej przestrzeni możliwe nauczenie klasyfikatora SVM z $C=\\infty$? (Możesz spróbować, ale nie musisz)\n",
    "7. Klasyfikator SVM jest binarny. Jak mógłbyś go użyć do klasyfikacji danych wieloklasowych?\n",
    "\n",
    "Odpowiedź na punkty 2., 5. i 7. wpisz poniżej (krótkie konkretne odowiedzi wystarczą), dla reszty pozostaw kod w komórkach i ich wyniki jako dowód Twoich eksperymentów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Niedouczenie dla: 0.01 i 0.1. Przeuczenie dla 10\n",
    "\n",
    "5 Nie jest odporny, nieznormalizowane skale mogą negatywnie wpłynąć na dopór najlepszego marginesu\n",
    "\n",
    "7 Wytrenować wiele klasyfikatorów każdy do odseparowania jednej klasy od wszystkich pozostałych, następnie przy klasyfikacji wybrać ten który jest najbardziej pewny (predict_proba), albo podzielić zbiór na wiele zbiorów gdzie w każdym mamy tylko dwie klasy, następnie wytrenować svm na wszystkich tych zbiorach, przy klasyfikacji wybieramy klasę która jest wybierana najczęściej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 4. - Wybór parametrów (za 20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygeneruj dane z szachownicą (takie same jak w zadanich wyżej)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(2000,2)*20\n",
    "y = np.sin(X[:,0])+np.sin(X[:,1])>0\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spróbuj wybrać optymalne parametry dla klasyfikatora SVM z jądrem RBF (weź pod uwagę parametr `gamma` jądra i parametr `C`). Przykładową techniką poszukiwania dobrych parametrów jest zdefiniowanie tzw. siatki parametrów i sprawdzenie wszystkich możliwości. W pierwszym kroku musimy ustalić siatkę czyli np. dla parametru `gamma` będziemy testować wartości 0.1, 1, 10, a dla parametru `C` wartości 1 i 10. Poszukiwanie gridowe dla takiej siatki uruchomi proces uczenia 6 razy, zewaluuje trafność i zwróci najlepsze parametry. Ewaluacja trafności może się odbywać np. walidacją krzyżową, a w `sklearn` jest to zaimpementowane w obiekcie `GridSearchCV`.\n",
    "\n",
    "Pierwszym argumentem konstruktora obiektu `GridSearchCV` jest obiekt klasyfikatora, a kolejne ważne parametry to ` param_grid= słownik z siatką`, `scoring='accuracy'` (aby była ewaluowana trafność). Dodatkowo można ustawić `verbose=2` jeśli chcesz, żeby pojawiły się na konsoli dodatkowe informacji o uczeniu kolejnych klasyfikatorów oraz parametrem `cv=` kontrulującym na ile cześci powinien zostać podzielony zbiór uczący w trakcie walidacji krzyżowej. Np.\n",
    "```\n",
    "GridSearchCV(KLASYFIKATOR, param_grid= PARAMETRY scoring='accuracy', verbose=2, cv=3)\n",
    "```\n",
    "Siatkę `PARAMETRY` definiujemy poprzez podanie słownika, którego kluczami są nazwy parametrów klasyfikatora, a wartościami są listy wartości do sprawdzenia np. `{'gamma':[0.1,1,10], 'C':[1,10]}`. `GridSearchCV` uruchamiamy standardowo czyli porzez funkcję `fit()`. \n",
    "\n",
    "Znajdź dobre ustawienia klasyfikatora SVM z jądrem RBF dla problemu z szachownicą."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44f0e6dd8435c0f831fb93107a2fa52c",
     "grade": true,
     "grade_id": "cell-9c4cce5f5fd7efe4",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel='rbf')\n",
    "params = dict(gamma=[0.1,1,10], C=[1,10])\n",
    "grid = GridSearchCV(model, param_grid=params, scoring='accuracy', cv=3, n_jobs=4)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyjrzyj się wynikowi - właściwość `GRID.best_params_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oraz szczegółom przeszukiwania - właściwość `GRID.cv_results_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wytrenuj klasyfikator z najlepszymi znalezionymi parametrami i zwizualizuj go `plot_classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bf8a87c7b37f452b4519e07c7efcd98",
     "grade": false,
     "grade_id": "cell-8019862b94860ac1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf', **grid.best_params_)\n",
    "model.fit(X, y)\n",
    "plot_classifier(X, y, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.2)\n",
    "\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "  {'C': [1, 10, 100, 1000], 'degree': [2, 3, 4], 'kernel': ['poly']},\n",
    " ]\n",
    "\n",
    "model = SVC()\n",
    "grid = GridSearchCV(model, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=4)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polecenia**\n",
    "1. Typ funkcji jądrowej może być jednym z parametrów które stroimy. Jednak różne funkcje jądrowe mają różne parametry - jak to wyspecyfikować? Zobacz fragment [dokumentacji sekcja 3.2.1 ](https://scikit-learn.org/stable/modules/grid_search.html#exhaustive-grid-search), który pokazuje jak to zrobić. Rozszerz twoje przeszukiwanie o jądra liniowe i wielomianowe.\n",
    "2. Spróbuj strojenia na innym wybranym zbiorze.\n",
    "3. Czy wynik uzyskany przez GridSearchCV jest dobrą estymacją trafności na zbiorze testowym?\n",
    "\n",
    "Odpowiedź na punkt 3. poniżej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Tak, ponieważ używa walidacji krzyżowej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 5. - SVM dla dużych danych (za 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_wine\n",
    "X, y = get_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metoda maszyn wektorów podpierających ma wiele zalet. Po pierwsze tworzy modele z dość małą liczbą wektorów podpierających które determinują wielkość nauczonego modelu. Dodatkowo faza predykcji jest bardzo szybka. Ponieważ uczenie jest zależne tylko od kilku, automatycznie wybieranych, wektorów podpierających, a resztę zbioru danych można by usunąć bez zmiany wyindukowanego klasyfikatora to wynik SVM zwykle zależy od małego podzbioru zbioru uczącego. SVM dobrze sobie radzą nawet w sytuacji gdy liczba przykładów jest dalece mniejsza niż liczba cech! (Przypomnij sobie co się wtedy dzieje z regresją liniową). Dodatkowo, pomimo tego że SVM jest klasyfikatorem liniowym to zastosowanie triku jądrowego pozwala na indukcje płaszczyzny separującej w rozszerzonej przestrzeni. Ponadto trik jądrowy pozwala na tworzenie własnych jąder dostosowanych do dziedziny problemu (tak jak w kNN mogliśmy projektować funkcje odległości) co spowodowało  szerokie ich wykorzystanie w tekstach czy obrazach (istnieją gotowe funkcje jądrowe nawet do przetwarzania grafów).\n",
    "\n",
    "Jeśli chodzi o najważniejsze wady tego klasyfikatora to pierwszą dużą wadą jest konieczność strojenia parametrów funkcji jądrowych oraz parametru $C$. Jednakże w świetle powszechnego korzystania z sieci neuronowych, które wymagają strojenia znacznie większej liczby parametrów (architektura, różne dodatkowe triki) nie wiem czy nadal powinno się to uważać za  wadę nie do przeskoczenia. Druga wada SVM jest jednak znaczenie gorsza: koszt obliczeniowy uczenia. Rozwiązywanie problemu programowania kwadratowego definiowanego przez SVM jest rzędu sześciennego od liczby przykładów $\\mathcal{O}[n^3]$ czy dla efektywniejszych implementacji $\\mathcal{O}(\\max(n,d) \\min (n,d)^2)$  - nadal jest to bardzo kosztowne obliczeniowo.\n",
    "\n",
    "Jednakże na ten drugi problem możemy rozwiązać za pomocą algorytmów w rodzaju stochastycznego spadku wzdłuż gradientu! Funkcja celu która jest optymalizowana przez SVM nazywamy funkcją zwiasową (*hinge loss*) i jest ona ekwiwalentna z prymalną postacią problemu. Jednak to właśnie jego postać dualna umożliwiała zastosowanie triku jądrowego. Aby więc zastosować SGD trzeba by rzeczywiście przeskalować zbiór uczący do przestrzeni cech definiowanej przez jądro (ona może mieć nawet $\\infty$ liczbę wymiarów!). Na szczęście tak jak SGD używa przybliżonej estymacji gradientu tak samo możemy użyć *przybliżonej* transformacji jądrowej. Jedną z takich technik jest aproksymacja Nyströma, która wychodzi z założenia że macierz jądrowa jest rzędu $m$ dalece mniejszego od $n$. Zakłada się więc, że mając tylko wiedzę o $m$ wierszach macierzy jądrowej można zrekonstruować ją całą. W praktyce nie jest to prawda, ale po prostu mamy nadzieję, że $n-m$ najmniejszych wartości własnych jest na tyle małe że i tak uzyskujemy dobrą aproksymację (patrz: Materiały dla chętnych).\n",
    "\n",
    "Wytrenuj na wczytanych danych SVM poprzez algorytm SGD - obiekt `SGDClassifier`. Pamiętaj o wyspecyfikowaniu zawiasowej funkcji straty `loss='hinge'`. Dodatkowo, aby wykonać trik jądrowy przetransformuj dane wejściowe aprokymacją Nyströma (obiekt `Nystroem`). Parametry tego obiektu to `kernel` (ustawmy RBF) oraz `gamma` (dla RBF), a także ` n_components` czyli ile wektorów używanych jest do przybliżenia jądra (domyślnie 100 i do naszych celów jest to wystarczające)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17bcf9da9c311672656d62efaeea9d06",
     "grade": false,
     "grade_id": "cell-94b7ee474dce39f2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2)\n",
    "nyst = Nystroem(kernel='rbf', n_components=100)\n",
    "X_tr_nyst = nyst.fit_transform(X_tr)\n",
    "X_te_nyst = nyst.fit_transform(X_te) # tu powinno być samo transform chyba?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównaj czas uczenia na tych samych danych modelu `SVC` oraz modelu przybliżonego (`Nystroem`+`SGD`). Czas wykonania komórki kodu możesz przetestować poprzez dodanie do niej na początku znacznika \n",
    "`%%timeit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2058ecbb466c9e6b9aa1fd6ef08369a",
     "grade": false,
     "grade_id": "cell-09fd33181b12824c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "model = SVC(kernel='rbf')\n",
    "model.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4911d4ab4a0b943a015deebc2b52715f",
     "grade": false,
     "grade_id": "cell-d6292d0164a66320",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "model = SGDClassifier(loss='hinge')\n",
    "model.fit(X_tr_nyst, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_factor = 5\n",
    "X_repeat = np.repeat(X, repeat_factor).reshape([X.shape[0] * repeat_factor, X.shape[1]])\n",
    "y_repeat = np.repeat(y, repeat_factor).reshape(np.array(y.shape)*repeat_factor)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_repeat, y_repeat, test_size=0.2)\n",
    "nyst = Nystroem(kernel='rbf', n_components=100)\n",
    "X_tr_nyst = nyst.fit_transform(X_tr)\n",
    "X_te_nyst = nyst.transform(X_te) # tutaj bez fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "model_svc = SVC(kernel='rbf')\n",
    "model_svc.fit(X_tr, y_tr)\n",
    "model_svc.score(X_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "model_sgd = SGDClassifier(loss='hinge')\n",
    "model_sgd.fit(X_tr_nyst, y_tr)\n",
    "model_sgd.score(X_te_nyst, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "repeat_factor = 5\n",
    "X_repeat = np.repeat(X, repeat_factor).reshape([X.shape[0] * repeat_factor, X.shape[1]])\n",
    "y_repeat = np.repeat(y, repeat_factor).reshape(np.array(y.shape)*repeat_factor)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_repeat, y_repeat, test_size=0.2)\n",
    "scaler = MinMaxScaler()\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_te = scaler.transform(X_te)\n",
    "nyst = Nystroem(kernel='rbf', n_components=100)\n",
    "X_tr_nyst = nyst.fit_transform(X_tr)\n",
    "X_te_nyst = nyst.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "model_svc = SVC(kernel='rbf')\n",
    "model_svc.fit(X_tr, y_tr)\n",
    "model_svc.score(X_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "model_sgd = SGDClassifier(loss='hinge')\n",
    "model_sgd.fit(X_tr_nyst, y_tr)\n",
    "model_sgd.score(X_te_nyst, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "repeat_factor = 5\n",
    "X_repeat = np.repeat(X, repeat_factor).reshape([X.shape[0] * repeat_factor, X.shape[1]])\n",
    "y_repeat = np.repeat(y, repeat_factor).reshape(np.array(y.shape)*repeat_factor)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_repeat, y_repeat, test_size=0.2)\n",
    "\n",
    "for n_components in [10, 50, 100, 200, 300, 1000]:\n",
    "    nyst = Nystroem(kernel='rbf', n_components=n_components)\n",
    "    X_tr_nyst = nyst.fit_transform(X_tr)\n",
    "    X_te_nyst = nyst.transform(X_te)\n",
    "\n",
    "    start = time.time()\n",
    "    model_sgd.fit(X_tr_nyst, y_tr)\n",
    "    end = time.time()\n",
    "    print(f'n_components: {n_components}, score: {model_sgd.score(X_te_nyst, y_te)}, time: {end - start:.02f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polecenia**\n",
    "1. Zwiększ rozmiar zbioru pięciokrotnie (kopiując przykłady). Jak teraz wygląda czas uczenia klasyfikatora  obiema metodami?\n",
    "2. Porównaj trafności obydwu metod na zbiorze uczącym i testowym.\n",
    "3. Spróbuj przeskalować dane uczące przed użyciem obydwu metod np. metodą Min-Max\n",
    "   ```\n",
    "   from sklearn.preprocessing import MinMaxScaler\n",
    "   scaler = MinMaxScaler()\n",
    "   ```\n",
    "   Jak zmienił się czas uczenia? Jak zmieniła się trafność modelu?\n",
    "\n",
    "4. Przetestuj różne wartości `n_components` transformacji przybliżonej. Spróbuj zaobserwować zależności pomiędzy trafnością modelu a modelem referencyjnym (bez przybliżenia). Możesz też spróbować porównać czasy uczenia dla kliku wielkości `n_components`.\n",
    "\n",
    "Zostaw kod w komórkach i ich wyniki na dowód Twoich eksperymentów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SVC: 17.2s, SGDClassifier: 117ms\n",
    "2. SVC: 0.80, SGDClassifier: 0.81\n",
    "3. SVC: [14s, 0.80] (czas się skrócił, trafność ta sama), SGDClassifier: [141ms, 0.75] (czas się wydłużył, a trafność zmniejszyła)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
